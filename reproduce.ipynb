{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64daa581",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy, os\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from torch_geometric.nn import to_hetero\n",
    "from torch_geometric.transforms import Compose, ToUndirected\n",
    "from torch_geometric.loader import NeighborLoader, LinkNeighborLoader\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "# project-specific\n",
    "import lib\n",
    "from lib.model import SupervisedNodePredictions, SupervisedEdgePredictions, SupervisedMTL, train_node_readout, make_gae, make_gmae, Readout, test_node_readout, get_x_dict, GraphSAGE, pretrain_gmae, train_edge_readout, test_edge_readout\n",
    "\n",
    "from lib.dataset import load_data, to_inductive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e24878",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8d82fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "root_path = 'OGBN-MAG/'\n",
    "transform = Compose([ToUndirected(merge=False)])\n",
    "preprocess = 'metapath2vec'\n",
    "data = lib.dataset.load_data(root_path, transform=transform, preprocess=preprocess)\n",
    "\n",
    "train_data = data.subgraph({ # used for the unsupervised part\n",
    "    \"paper\": data[\"paper\"].train_mask.nonzero(as_tuple=False).view(-1)\n",
    "})\n",
    "num_classes = int(data[\"paper\"].y.max()) + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a844420",
   "metadata": {},
   "source": [
    "Pretrain the unsupervised encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0ec4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = pretrain_gmae(train_data)\n",
    "torch.save(encoder.state_dict(), \"gmae_encoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf894384",
   "metadata": {},
   "source": [
    "Run the trained encoder on the full data for training the node classification and edge prediction readout layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8a66c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_to_loader(d):\n",
    "    x_dict = get_x_dict(d)\n",
    "    x_dict = {k: v.to(device) for k, v in x_dict.items()}\n",
    "    with torch.no_grad():\n",
    "        z_dict = encoder(x_dict, d.edge_index_dict)\n",
    "    z_paper = z_dict[\"paper\"]\n",
    "    z_paper = z_paper.detach().cpu()\n",
    "    y_paper = d[\"paper\"].y.cpu()\n",
    "    torchdataset = torch.utils.data.TensorDataset(z_paper, y_paper)\n",
    "    return torch.utils.data.DataLoader(\n",
    "        torchdataset,\n",
    "        batch_size=16,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "train_loader = dataset_to_loader(train_data)\n",
    "val_loader   = dataset_to_loader(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11da3e52",
   "metadata": {},
   "source": [
    "Train node readout "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab3d952",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.eval()\n",
    "readout = Readout(num_classes).to(device)\n",
    "for epoch in range(5):\n",
    "    loss = train_node_readout(readout, train_loader)\n",
    "    acc  = test_node_readout(readout, val_loader)\n",
    "    print(f\"Epoch {epoch:02d} | Loss: {loss:.4f} | Val Acc: {acc:.4f}\")\n",
    "\n",
    "torch.save(readout.state_dict(), \"gmae_node_classification_readout\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64691386",
   "metadata": {},
   "source": [
    "Train edge readout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c443864c",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_train_mask = data[\"paper\"].train_mask\n",
    "paper_test_mask  = data[\"paper\"].val_mask\n",
    "edge_type = (\"paper\", \"has_topic\", \"field_of_study\")\n",
    "edge_index = data[edge_type].edge_index   \n",
    "paper_idx = edge_index[0]               \n",
    "fos_idx = edge_index[1]              \n",
    "train_edge_mask = paper_train_mask[paper_idx]\n",
    "test_edge_mask = paper_test_mask[paper_idx]\n",
    "train_edge_index = edge_index[:, train_edge_mask]\n",
    "test_edge_index  = edge_index #[:, test_edge_mask] allowed to see all edges during inference \n",
    "\n",
    "x_dict = get_x_dict(data)\n",
    "with torch.no_grad():\n",
    "    z_dict = encoder(x_dict, data.edge_index_dict)\n",
    "z_paper = z_dict[\"paper\"].detach()            \n",
    "z_fos   = z_dict[\"field_of_study\"].detach()  \n",
    "readout = Readout(1) # single out channel -- probability\n",
    "\n",
    "def edge_index_to_loader(edge_index, z_paper, z_fos, batch_size=1024):\n",
    "    pos_edge_index = edge_index\n",
    "    num_pos = pos_edge_index.size(1)\n",
    "    num_paper = z_paper.size(0)\n",
    "    num_fos   = z_fos.size(0)\n",
    "    neg_edge_index = negative_sampling(\n",
    "        pos_edge_index,\n",
    "        num_nodes=(num_paper, num_fos),\n",
    "        num_neg_samples=num_pos,\n",
    "    )\n",
    "    z_src_pos = z_paper[pos_edge_index[0]]\n",
    "    z_dst_pos = z_fos[pos_edge_index[1]]  \n",
    "    z_src_neg = z_paper[neg_edge_index[0]]\n",
    "    z_dst_neg = z_fos[neg_edge_index[1]]  \n",
    "    z_src = torch.cat([z_src_pos, z_src_neg], dim=0)\n",
    "    z_dst = torch.cat([z_dst_pos, z_dst_neg], dim=0)\n",
    "    y = torch.cat([\n",
    "        torch.ones(num_pos, dtype=torch.float32),\n",
    "        torch.zeros(num_pos, dtype=torch.float32),\n",
    "    ], dim=0)\n",
    "    dataset = TensorDataset(z_src, z_dst, y)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(\"building edge datasets...\")\n",
    "train_loader = edge_index_to_loader(train_edge_index, z_paper, z_fos)\n",
    "test_loader  = edge_index_to_loader(test_edge_index,  z_paper, z_fos)\n",
    "\n",
    "print(\"training edge predictor...\")\n",
    "for epoch in range(2):\n",
    "    loss = train_edge_readout(readout, train_loader)\n",
    "    acc  = test_edge_readout(readout, test_loader)\n",
    "    print(f\"Epoch {epoch:02d} | Loss: {loss:.4f} | Test Acc: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2614cab",
   "metadata": {},
   "source": [
    "# Train supervised versions of the network for comparison\n",
    "## Node classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda3d90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_type = \"paper\"\n",
    "data_inductive = to_inductive(copy.deepcopy(data), target_type)\n",
    "\n",
    "train_loader = NeighborLoader(\n",
    "    data_inductive,\n",
    "    input_nodes=(target_type, data_inductive[target_type].train_mask),\n",
    "    num_neighbors=[15, 10],\n",
    "    batch_size=2048,\n",
    "    shuffle=True,\n",
    ")\n",
    "val_loader = NeighborLoader(\n",
    "    data,\n",
    "    input_nodes=(target_type, data[target_type].val_mask),\n",
    "    num_neighbors=[15, 10],\n",
    "    batch_size=2048,\n",
    ")\n",
    "\n",
    "hidden_dim = 128\n",
    "num_classes = int(data_inductive[target_type].y.max()) + 1\n",
    "\n",
    "model = GraphSAGE(in_channels=hidden_dim, out_channels=hidden_dim, num_classes=num_classes)\n",
    "model = to_hetero(model, data_inductive.metadata(), aggr='sum')\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n",
    "pipeline = SupervisedNodePredictions(\n",
    "    model=model,\n",
    "    device=device,\n",
    "    optimizer=optimizer,\n",
    "    target_type=target_type\n",
    ")\n",
    "\n",
    "best_acc = 0\n",
    "save_path = 'supervised/models/node/'\n",
    "for epoch in range(10):\n",
    "    loss = pipeline.train(train_loader)\n",
    "    acc = pipeline.test(val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | \"\n",
    "          f\"Loss: {loss:.4f} | \"\n",
    "          f\"Val Acc: {acc:.4f}\")\n",
    "\n",
    "    torch.save({\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optimizer_state\": optimizer.state_dict(),\n",
    "        \"epoch\": epoch\n",
    "    }, save_path+f\"checkpoint_{epoch}.pt\")\n",
    "\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        torch.save(model.state_dict(), save_path+\"best.pt\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Link predictions",
   "id": "742ecd17d1f33656"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238b171f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_edge_type = ('paper', 'has_topic', 'field_of_study')\n",
    "\n",
    "train_loader = LinkNeighborLoader(\n",
    "    data_inductive,\n",
    "    num_neighbors=[15, 10],\n",
    "    edge_label_index=(target_edge_type, data_inductive[target_edge_type].edge_index),\n",
    "    neg_sampling_ratio=1.0,\n",
    "    batch_size=2048,\n",
    "    shuffle=True,\n",
    ")\n",
    "edge_index_all = data[target_edge_type].edge_index\n",
    "src_papers = edge_index_all[0]\n",
    "\n",
    "# Use papers val_mask to select validation edges\n",
    "val_edge_mask = data['paper'].val_mask[src_papers]\n",
    "val_edge_index = edge_index_all[:, val_edge_mask]\n",
    "\n",
    "val_loader = LinkNeighborLoader(\n",
    "    data,\n",
    "    num_neighbors=[15, 10],\n",
    "    edge_label_index=(target_edge_type, val_edge_index),\n",
    "    neg_sampling_ratio=1.0,\n",
    "    batch_size=2048,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "hidden_dim = 128\n",
    "\n",
    "model = GraphSAGE(in_channels=hidden_dim, out_channels=hidden_dim)\n",
    "model = to_hetero(model, data_inductive.metadata(), aggr='sum')\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n",
    "pipeline = SupervisedEdgePredictions(\n",
    "    model=model,\n",
    "    device=device,\n",
    "    optimizer=optimizer,\n",
    "    target_edge_type=target_edge_type\n",
    ")\n",
    "\n",
    "best_auc = 0\n",
    "save_path = \"supervised/models/edge/\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "max_steps = 10_000\n",
    "eval_every = 500\n",
    "save_every = 500\n",
    "\n",
    "step = 0\n",
    "epoch = 0\n",
    "\n",
    "pbar = tqdm(total=max_steps)\n",
    "while step < max_steps:\n",
    "    print(f\"=== Epoch {epoch} ===\")\n",
    "    for batch in train_loader:\n",
    "        step += 1\n",
    "\n",
    "        loss = pipeline.train_on_batch(batch)\n",
    "\n",
    "        if step % eval_every == 0:\n",
    "            metrics = pipeline.test(val_loader)\n",
    "            auc = metrics[\"AUC\"]\n",
    "\n",
    "            pbar.write(\n",
    "                f\"AUC: {auc:.4f}\"\n",
    "            )\n",
    "\n",
    "            if auc > best_auc:\n",
    "                best_auc = auc\n",
    "                torch.save(model.state_dict(), save_path + \"best.pt\")\n",
    "\n",
    "        if step % save_every == 0:\n",
    "            torch.save({\n",
    "                \"model_state\": model.state_dict(),\n",
    "                \"optimizer_state\": optimizer.state_dict(),\n",
    "                \"step\": step,\n",
    "                \"epoch\": epoch,\n",
    "            }, save_path + f\"checkpoint_{step}.pt\")\n",
    "\n",
    "        if step >= max_steps:\n",
    "            break\n",
    "\n",
    "        pbar.update(1)\n",
    "    epoch += 1"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Multi-Task Learning",
   "id": "8a79b0b5794d2f25"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "target_node_type = \"paper\"\n",
    "\n",
    "train_loader = LinkNeighborLoader(\n",
    "    data_inductive,\n",
    "    num_neighbors=[15, 10],\n",
    "    edge_label_index=(target_edge_type,  data_inductive[target_edge_type].edge_index),\n",
    "    neg_sampling_ratio=1.0,\n",
    "    batch_size=2048,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "edge_index_all = data[target_edge_type].edge_index\n",
    "src_papers = edge_index_all[0]\n",
    "\n",
    "# Use papers val_mask to select validation edges\n",
    "val_edge_mask = data['paper'].val_mask[src_papers]\n",
    "val_edge_index = edge_index_all[:, val_edge_mask]\n",
    "\n",
    "val_loader = LinkNeighborLoader(\n",
    "    data,\n",
    "    num_neighbors=[15, 10],\n",
    "    edge_label_index=(target_edge_type, val_edge_index),\n",
    "    neg_sampling_ratio=1.0,\n",
    "    batch_size=2048,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "model = GraphSAGE(in_channels=hidden_dim, out_channels=hidden_dim, num_classes=num_classes)\n",
    "model = to_hetero(model, data_inductive.metadata(), aggr='sum')\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "pipeline = SupervisedMTL(\n",
    "    model=model,\n",
    "    device=device,\n",
    "    optimizer=optimizer,\n",
    "    target_node_type=target_node_type,\n",
    "    target_edge_type=target_edge_type\n",
    ")\n",
    "\n",
    "best_loss = float(\"inf\")\n",
    "save_path = 'supervised/models/mtl/'\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "max_steps = 10_000\n",
    "eval_every = 500\n",
    "save_every = 500\n",
    "\n",
    "step = 0\n",
    "epoch = 0\n",
    "\n",
    "pbar = tqdm(total=max_steps)\n",
    "while step < max_steps:\n",
    "    print(f\"=== Epoch {epoch} ===\")\n",
    "    for batch in train_loader:\n",
    "        step += 1\n",
    "\n",
    "        pipeline.train_on_batch(batch)\n",
    "\n",
    "        if step % eval_every == 0:\n",
    "            metrics = pipeline.test(val_loader)\n",
    "            loss = metrics[\"loss_total\"]\n",
    "\n",
    "            print(\n",
    "                f\"AUC: {metrics['AUC']:.4f} | \"\n",
    "                f\"Accuracy: {metrics['Accuracy']:.4f} | \"\n",
    "                f\"Edge loss: {metrics['loss_edge']:.4f} | \"\n",
    "                f\"Node loss: {metrics['loss_node']:.4f} | \"\n",
    "                f\"Total Loss: {loss:.4f}\"\n",
    "            )\n",
    "\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                torch.save(model.state_dict(), save_path + \"best.pt\")\n",
    "\n",
    "        if step % save_every == 0:\n",
    "            torch.save({\n",
    "                \"model_state\": model.state_dict(),\n",
    "                \"optimizer_state\": optimizer.state_dict(),\n",
    "                \"step\": step,\n",
    "                \"epoch\": epoch,\n",
    "            }, save_path + f\"checkpoint_{step}.pt\")\n",
    "\n",
    "        if step >= max_steps:\n",
    "            break\n",
    "\n",
    "        pbar.update(1)\n",
    "    epoch += 1"
   ],
   "id": "496b783ada461c14"
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
